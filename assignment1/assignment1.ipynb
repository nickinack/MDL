{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "b357b88907b5ce06b15bb5a84b98b54e0ba1dbedcbaf9c2ce2558773387c01e1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn"
   ]
  },
  {
   "source": [
    "## Task 2.2: Calculating bias and variance\n",
    "### 2.2 Resampling the data\n",
    "Importing the dataset from `data.pkl` as a (5000, 2) numpy array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5000, 2)\n(5000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('data.pkl', 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "print(dataset.shape)\n",
    "X_d, y_d = dataset[:,0], dataset[:, 1]\n",
    "print(X_d.shape, y_d.shape)\n",
    "# print('dataset: ', dataset[:3])\n",
    "# print('input, output', X_d[:3], y_d[:3])"
   ]
  },
  {
   "source": [
    "Splitting the dataset into 90:10 training-testing dataset as required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train (4500,) (4500,)\nTest (500,) (500,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_d, y_d, test_size=0.1, random_state=42)\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Set of training sets (10, 450, 2)\nTraining set #0 (450, 2)\n"
     ]
    }
   ],
   "source": [
    "ts = 10\n",
    "tsize = X_train.shape[0]\n",
    "tsels = X_train.shape[0] // ts\n",
    "training_sets = np.array([np.asmatrix((X_train[st:st+tsels], \n",
    "    y_train[st:st+tsels])).T for st in range(0, tsize, tsels)])\n",
    "# Or we can just have it as a set of tuples (x, y)\n",
    "# training_sets = np.array((X_train[st:st+tsels], \n",
    "#     y_train[st:st+tsels]) for st in range(0, tsize, tsels)])\n",
    "print('Set of training sets', training_sets.shape)\n",
    "print('Training set #0', training_sets[0].shape)"
   ]
  }
 ]
}